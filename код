import json
import requests
from vtapi3 import VirusTotalAPIUrls, VirusTotalAPIError
from urllib.parse import urlparse, urljoin
from bs4 import BeautifulSoup
import colorama

link = 'https://mail.ru/'
colorama.init()
GREEN = colorama.Fore.GREEN
RESET = colorama.Fore.RESET
YELLOW = colorama.Fore.YELLOW

internal_urls = set()
external_urls = set()

total_urls_visited = 0

def is_valid(url):
    parsed = urlparse(url)
    return bool(parsed.netloc) and bool(parsed.scheme)

def get_all_website_links(url):
    urls = set()
    domain_name = urlparse(url).netloc
    soup = BeautifulSoup(requests.get(url).content, "html.parser")

    for a_tag in soup.find_all("a"):
        href = a_tag.attrs.get("href")
        if href == "" or href is None:
            continue
        href = urljoin(url, href)
        parsed_href = urlparse(href)
        href = parsed_href.scheme + "://" + parsed_href.netloc + parsed_href.path

        if not is_valid(href):
            continue

        if href in internal_urls:
            continue

        if domain_name not in href:
            if href not in external_urls:
                print(f"{YELLOW}[!] Внешняя ссылка: {href}{RESET}")
                external_urls.add(href)
                check_link(href)
            continue

        print(f"{GREEN}[*] Внутренняя ссылка: {href}{RESET}")
        urls.add(href)
        internal_urls.add(href)

    return urls

def crawl(url, max_urls=30):
    global total_urls_visited
    total_urls_visited += 1
    print(f"{GREEN}[*] Проверено: {url}{RESET}")

    links = get_all_website_links(url)

    for link in links:
        if total_urls_visited > max_urls:
            break
        crawl(link, max_urls=max_urls)

def check_link(link):
    url_id = VirusTotalAPIUrls.get_url_id_base64(link)
    vt_api_urls = VirusTotalAPIUrls('1d4ef45a0f43c2f622b4046e283605f0a72f293c35e1a41dbdd732ba9252f2e7')

    try:
        result = vt_api_urls.upload(link)
    except VirusTotalAPIError as err:
        print(err, err.err_code)
    else:
        if vt_api_urls.get_last_http_error() == vt_api_urls.HTTP_OK:
            response = vt_api_urls.get_report(url_id)
            result = json.loads(response)

            if 'data' in result and 'attributes' in result['data'] and 'last_analysis_stats' in result['data']['attributes']:
                if result['data']['attributes']['last_analysis_stats']['malicious'] > 0:
                    descript = result['data']['attributes']['html_meta']['description'][0]
                    reputation = result['data']['attributes']['reputation']
                    malicious = result['data']['attributes']['last_analysis_stats']['malicious']
                    undetected = result['data']['attributes']['last_analysis_stats']['undetected']
                    harmless = result['data']['attributes']['last_analysis_stats']['harmless']

                    print(f"Проверяемая ссылка: {link} - Вредоносная")
                    print("Описание:", descript)
                    print("Репутация:", reputation)
                    print("Считают вредной:", malicious)
                    print("Воздержались:", undetected)
                    print("Считают безвредной:", harmless)
                else:
                 print(f"Проверяемая ссылка: {link} - безопасна")
            else:
                print("Ошибка при получении результата для ссылки:", link)
        else:
            print('HTTP Error [' + str(vt_api_urls.get_last_http_error()) +']')

if __name__ == "__main__":
    url = 'https://mail.ru/'
    max_urls = 30

    crawl(url, max_urls=max_urls)

    print("[+] Total Internal links:", len(internal_urls))
    print("[+] Total External links:", len(external_urls))
    print("[+] Total URLs:", len(external_urls) + len(internal_urls))
    print("[+] Total crawled URLs:", max_urls)

    domain_name = urlparse(url).netloc
